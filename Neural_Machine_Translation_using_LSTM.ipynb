{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from helper import load_variable, save_variable\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from config import Weights_DIR\n",
        "from dataset import prepare_data\n",
        "from models.lstm.model import LSTMModel\n",
        "max_sentence_length = 16\n",
        "train_df = prepare_data(type='train', max_entries=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "give your application an accessibility workout \n",
            "(1, 6, 256)\n",
            " स्रोत स्रोत विस्तारः अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने\n",
            "(1, 16, 256)\n",
            " by and and current current current current current current event event event event event event event\n"
          ]
        }
      ],
      "source": [
        "class Translate:\n",
        "    def __init__(self, path_to_weights=Weights_DIR, data=None, max_sentence_length=16, embedding_size=256):\n",
        "        if path_to_weights is not None:\n",
        "            self.path_to_weights = path_to_weights\n",
        "        \n",
        "        if data is not None:\n",
        "            self.data = data\n",
        "        else:\n",
        "            print(\n",
        "                \"to instantiate the translation pass the path to weights or pass the training data\")\n",
        "            assert False\n",
        "        self.en_hi_model = None\n",
        "        self.hi_en_model = None\n",
        "        self.english_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "            filters='', oov_token='<OOV>', lower=False)\n",
        "        self.hindi_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "            filters='', oov_token='<OOV>', lower=False)\n",
        "        self.english_vocab_size = None\n",
        "        self.hindi_vocab_size = None\n",
        "        self.validation_split = 0.15\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "\n",
        "    def load_weights(self):\n",
        "        if os.path.exists(os.path.join(self.path_to_weights, 'english_tokenizer')):\n",
        "            self.english_tokenizer = load_variable(\n",
        "                os.path.join(self.path_to_weights, 'english_tokenizer'))\n",
        "            self.english_vocab_size = len(\n",
        "                self.english_tokenizer.word_index) + 1\n",
        "        else:\n",
        "            print('Could not find english tokenizer weights')\n",
        "\n",
        "        if os.path.exists(os.path.join(self.path_to_weights, 'hindi_tokenizer')):\n",
        "            self.hindi_tokenizer = load_variable(\n",
        "                os.path.join(self.path_to_weights, 'hindi_tokenizer'))\n",
        "            self.hindi_vocab_size = len(self.hindi_tokenizer.word_index)+1\n",
        "        else:\n",
        "            print(\"Could not find hindi_tokenizer weights\")\n",
        "\n",
        "        if os.path.exists(os.path.join(self.path_to_weights, 'hi_en_model.h5')):\n",
        "            self.hi_en_model = LSTMModel(encoder_vocab_size=self.hindi_vocab_size, decoder_vocab_size=self.english_vocab_size)\n",
        "            self.hi_en_model(np.array([[[1]*self.max_sentence_length], [[1]*self.max_sentence_length]]))\n",
        "            self.hi_en_model.load_weights(os.path.join(self.path_to_weights, 'hi_en_model.h5'))\n",
        "        else:\n",
        "            print(\"could not load hi_en_model\")\n",
        "\n",
        "        if os.path.exists(os.path.join(self.path_to_weights, 'en_hi_model.h5')):\n",
        "            self.en_hi_model = LSTMModel(encoder_vocab_size=self.english_vocab_size, decoder_vocab_size=self.hindi_vocab_size)\n",
        "            self.en_hi_model(np.array([[[1]*self.max_sentence_length], [[1]*self.max_sentence_length]]))\n",
        "            self.en_hi_model.load_weights(os.path.join(self.path_to_weights, 'en_hi_model.h5'))\n",
        "        else:\n",
        "            print('Could not load en_hi_model')\n",
        "\n",
        "    def tokenize_data(self):\n",
        "        print(\"filtering the sentences on the basis of their length\")\n",
        "        data = self.data\n",
        "        data = data[data['en'].apply(\n",
        "            lambda x: len(x.split()) < max_sentence_length)]\n",
        "        data = data[data['hi'].apply(\n",
        "            lambda x: len(x.split()) < max_sentence_length)]\n",
        "        print(\n",
        "            f\"{data.__len__()} sentence pairs are valid or less than max_sentence_length\")\n",
        "        english_sentences = self.data['en'].to_list()\n",
        "        hindi_sentences = self.data['hi'].to_list()\n",
        "\n",
        "        print(\"start tokenization..\")\n",
        "        self.english_tokenizer.fit_on_texts(english_sentences)\n",
        "        self.hindi_tokenizer.fit_on_texts(hindi_sentences)\n",
        "        self.english_sequences = self.english_tokenizer.texts_to_sequences(\n",
        "            english_sentences)\n",
        "        self.hindi_sequences = self.hindi_tokenizer.texts_to_sequences(\n",
        "            hindi_sentences)\n",
        "\n",
        "        self.english_vocab_size = len(self.english_tokenizer.word_index) + 1\n",
        "        self.hindi_vocab_size = len(self.hindi_tokenizer.word_index) + 1\n",
        "        print(\"English Vocab Size: \", self.english_vocab_size)\n",
        "        print(\"Hindi Vocab Size: \", self.hindi_vocab_size)\n",
        "\n",
        "    def train_en_hi(self, num_epochs=10, optimizer='rmsprop', metrics=['accuracy'], **kwargs):\n",
        "\n",
        "        if self.en_hi_model == None:\n",
        "            self.en_hi_model = LSTMModel(encoder_vocab_size=self.english_vocab_size,\n",
        "                                         decoder_vocab_size=self.hindi_vocab_size)\n",
        "        encoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(self.english_sequences,\n",
        "                                                                       maxlen=max_sentence_length,\n",
        "                                                                       padding='post')\n",
        "\n",
        "        decoder_inputs = []\n",
        "        decoder_outputs = []\n",
        "        for sentence in self.hindi_sequences:\n",
        "            decoder_inputs.append(sentence[:-1])\n",
        "            decoder_outputs.append(sentence[1:])\n",
        "        decoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs,\n",
        "                                                                       maxlen=max_sentence_length,\n",
        "                                                                       padding='post')\n",
        "        decoder_outputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs,\n",
        "                                                                        maxlen=max_sentence_length,\n",
        "                                                                        padding='post')\n",
        "        self.en_hi_model.compile(optimizer=optimizer,\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                                 metrics=metrics)\n",
        "\n",
        "        callback1 = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=Weights_DIR+\"\\\\model\\\\\",\n",
        "            monitor='val_accuracy',\n",
        "            mode='max'\n",
        "        )\n",
        "        self.en_hi_model.fit([encoder_inputs, decoder_inputs],\n",
        "                             decoder_outputs,epochs=num_epochs ,validation_split=self.validation_split,\n",
        "                             callbacks=[callback1])\n",
        "\n",
        "    def train_hi_en(self, num_epochs=10, optimizer='rmsprop', metrics=['accuracy'], **kwargs):\n",
        "        if self.hi_en_model == None:\n",
        "            self.hi_en_model = LSTMModel(encoder_vocab_size=self.hindi_vocab_size,\n",
        "                                         decoder_vocab_size=self.english_vocab_size)\n",
        "        encoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(self.hindi_sequences,\n",
        "                                                                       maxlen=max_sentence_length,\n",
        "                                                                       padding='post')\n",
        "\n",
        "        decoder_inputs = []\n",
        "        decoder_outputs = []\n",
        "        for sentence in self.english_sequences:\n",
        "            decoder_inputs.append(sentence[:-1])\n",
        "            decoder_outputs.append(sentence[1:])\n",
        "        decoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs,\n",
        "                                                                       maxlen=max_sentence_length,\n",
        "                                                                       padding='post')\n",
        "        decoder_outputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs,\n",
        "                                                                        maxlen=max_sentence_length,\n",
        "                                                                        padding='post')\n",
        "        self.hi_en_model.compile(optimizer=optimizer,\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                                 metrics=metrics)\n",
        "\n",
        "        callback1 = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=Weights_DIR+\"\\\\model\\\\\",\n",
        "            monitor='val_accuracy',\n",
        "            mode='max'\n",
        "        )\n",
        "        self.hi_en_model.fit([encoder_inputs, decoder_inputs],\n",
        "                             decoder_outputs, epochs= num_epochs, validation_split=self.validation_split,\n",
        "                             callbacks=[callback1])\n",
        "                             \n",
        "    def train(self, model_to_train=None, num_epochs=2, optimizer='rmsprop', metrics=['accuracy'], **kwargs):\n",
        "        self.tokenize_data()\n",
        "        if model_to_train == 'en_hi':\n",
        "            self.train_en_hi(num_epochs=num_epochs,\n",
        "                             optimizer=optimizer, metrics=metrics, **kwargs)\n",
        "        elif model_to_train == 'hi_en':\n",
        "            self.train_hi_en(num_epochs=num_epochs,\n",
        "                             optimizer=optimizer, metrics=metrics, **kwargs)\n",
        "        else:\n",
        "            self.train_hi_en(num_epochs=num_epochs,\n",
        "                             optimizer=optimizer, metrics=metrics, **kwargs)\n",
        "            self.train_en_hi(num_epochs=num_epochs,\n",
        "                             optimizer=optimizer, metrics=metrics, **kwargs)\n",
        "\n",
        "    def save_model(self, path=None):\n",
        "        if path != None and os.path.exists(path):\n",
        "            self.path_to_weights = path\n",
        "        print(f\"saving model at {self.path_to_weights}\")\n",
        "\n",
        "        save_variable(self.english_tokenizer, os.path.join(\n",
        "            self.path_to_weights + \"\\\\english_tokenizer\"))\n",
        "        save_variable(self.hindi_tokenizer, os.path.join(\n",
        "            self.path_to_weights + \"\\\\hindi_tokenizer\"))\n",
        "        self.en_hi_model.save_weights(os.path.join(self.path_to_weights + \"\\\\en_hi_model.h5\"))\n",
        "        self.hi_en_model.save_weights(os.path.join(self.path_to_weights + \"\\\\hi_en_model.h5\"))\n",
        "\n",
        "\n",
        "\n",
        "    def translate_sentence_to_hindi(self,sentence):\n",
        "        if self.english_tokenizer ==None or self.hindi_tokenizer==None or self.en_hi_model==None:\n",
        "            print(\"the translate object is not initialized properly\")\n",
        "            return sentence\n",
        "        sequence = self.english_tokenizer.texts_to_sequences([sentence])\n",
        "        embedding = self.en_hi_model.encoder(np.array(sequence))\n",
        "        _, next_h, next_c= self.en_hi_model.encoder_lstm(embedding)\n",
        "        curr_token = [[0]]\n",
        "        curr_token[0][0] = self.hindi_tokenizer.word_index['<START>']\n",
        "        predict_sentence = ''\n",
        "        for i in range(max_sentence_length):\n",
        "                # print(curr_token, next_h.shape, next_c.shape)\n",
        "                temp = self.en_hi_model.decoder(np.array(curr_token))\n",
        "                output , next_h, next_c= self.en_hi_model.decoder_lstm(temp,initial_state= [next_h, next_c])\n",
        "                next_token = np.argmax(output[0, 0, :])\n",
        "                next_word =self.hindi_tokenizer.index_word[next_token]\n",
        "\n",
        "                if next_word == '<END>':\n",
        "                    break\n",
        "                predict_sentence =predict_sentence+\" \" + next_word\n",
        "                curr_token[0][0] = next_token\n",
        "                #curr_token[0].append(next_token)\n",
        "        return predict_sentence\n",
        "\n",
        "\n",
        "    def translate_sentence_to_english(self,sentence):\n",
        "        if self.english_tokenizer ==None or self.hindi_tokenizer==None or self.hi_en_model==None:\n",
        "            print(\"the translate object is not initialized properly\")\n",
        "            return sentence\n",
        "        sequence = self.hindi_tokenizer.texts_to_sequences([sentence])\n",
        "        embedding = self.hi_en_model.encoder(np.array(sequence))\n",
        "        _, next_h, next_c= self.hi_en_model.encoder_lstm(embedding)\n",
        "        curr_token = [[0]]\n",
        "        curr_token[0][0] = self.english_tokenizer.word_index['<START>']\n",
        "        predict_sentence = ''\n",
        "        for i in range(max_sentence_length):\n",
        "                # print(curr_token, next_h.shape, next_c.shape)\n",
        "                temp = self.hi_en_model.decoder(np.array(curr_token))\n",
        "                output , next_h, next_c= self.hi_en_model.decoder_lstm(temp,initial_state= [next_h, next_c])\n",
        "                next_token = np.argmax(output[0, 0, :])\n",
        "                next_word =self.english_tokenizer.index_word[next_token]\n",
        "\n",
        "                if next_word == '<END>':\n",
        "                    break\n",
        "                predict_sentence =predict_sentence+\" \" + next_word\n",
        "                curr_token[0][0] = next_token\n",
        "                #curr_token[0].append(next_token)\n",
        "        return predict_sentence\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # trans = Translate(path_to_weights=Weights_DIR, data=train_df)\n",
        "    # trans.train(num_epochs=1)\n",
        "    # trans.save_model()\n",
        "    trans1 = Translate(path_to_weights=Weights_DIR, data=train_df)\n",
        "    trans1.load_weights()\n",
        "    en ='give your application an accessibility workout ' \n",
        "    print(en)\n",
        "    hi = trans1.translate_sentence_to_hindi(en)\n",
        "    print(hi)\n",
        "    en = trans1.translate_sentence_to_english(hi)\n",
        "    print(en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<START> give your application an accessibility workout <END>'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['en'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UhoLQDkGgdx8"
      },
      "outputs": [],
      "source": [
        "#Make imports\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import seaborn as sns\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1k4_X3nGcl90"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQNeTtWZlwUc",
        "outputId": "d175be7b-ec38-4eda-a1c3-7da58dc12694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.3.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "0vLW3liTag9A",
        "outputId": "83f321f9-5934-4a1e-d2e8-97256a426020"
      },
      "outputs": [],
      "source": [
        "from dataset import prepare_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vD1-g3mhcj-U"
      },
      "outputs": [],
      "source": [
        "train = prepare_data(type='train', max_entries=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EjnK8XArDPUe"
      },
      "outputs": [],
      "source": [
        "#Some parameters\n",
        "vocab_size = 10000\n",
        "total_sentences = 25000\n",
        "maxlen = 10\n",
        "epochs = 70\n",
        "validation_split = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KyHjYB_eMpD7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10000it [00:00, 149256.58it/s]\n"
          ]
        }
      ],
      "source": [
        "en_data = []\n",
        "hi_data = []\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "for (en,hi) in tqdm(zip(train['en'].to_list(), train['hi'].to_list())):\n",
        "  l = min(len(en.split()), len(hi.split()))\n",
        "  if l <= maxlen:\n",
        "    en_data.append(en)\n",
        "    hi_data.append(hi)\n",
        "    cnt += 1\n",
        "  if cnt == total_sentences:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwR2rlOvB-xZ",
        "outputId": "d3c50d8c-7931-4c03-bd79-c91167ca33cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Vocab Size:  1428\n",
            "Hindi Vocab Size:  1849\n"
          ]
        }
      ],
      "source": [
        "#Tokenize the texts and convert to sequences\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
        "en_tokenizer.fit_on_texts(en_data)\n",
        "en_sequences = en_tokenizer.texts_to_sequences(en_data)\n",
        "\n",
        "hi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
        "hi_tokenizer.fit_on_texts(hi_data)\n",
        "hi_sequences = hi_tokenizer.texts_to_sequences(hi_data)\n",
        "\n",
        "english_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "hindi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
        "print(\"English Vocab Size: \", english_vocab_size)\n",
        "print(\"Hindi Vocab Size: \", hindi_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "luUa7TE6RYFq"
      },
      "outputs": [],
      "source": [
        "#Prepare encoder data\n",
        "encoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(en_sequences, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "olIjin62TPF7"
      },
      "outputs": [],
      "source": [
        "#Prepare decoder data\n",
        "decoder_inputs = []\n",
        "decoder_outputs = []\n",
        "\n",
        "for hi in hi_sequences:\n",
        "  decoder_inputs.append(hi[:-1])\n",
        "  decoder_outputs.append(hi[1:])\n",
        "\n",
        "decoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs, maxlen=maxlen, padding='post')\n",
        "decoder_outputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5OWbUrPRENr",
        "outputId": "7bd258d5-f4bc-4be0-ba91-fbf02a4ef838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9549, 10) (9549, 10) (9549, 10)\n"
          ]
        }
      ],
      "source": [
        "# Training and Testing split\n",
        "# 95%, 5%\n",
        "split = int(0.95 * total_sentences)\n",
        "\n",
        "X_train = [encoder_inputs[:split], decoder_inputs[:split]]\n",
        "y_train = decoder_outputs[:split]\n",
        "\n",
        "# Test data to evaluate our NMT model using BLEU score\n",
        "X_test = en_data[:split]\n",
        "y_test = hi_data[:split]\n",
        "\n",
        "print(X_train[0].shape, X_train[1].shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUboXx8WjDnP",
        "outputId": "45327162-4c97-471f-c49a-e0694a2ef0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    365568      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    473344      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 1849)   475193      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,364,729\n",
            "Trainable params: 2,364,729\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Define LSTM model\n",
        "d_model = 256\n",
        "\n",
        "#Encoder\n",
        "inputs = tf.keras.layers.Input(shape=(None,))\n",
        "x = tf.keras.layers.Embedding(english_vocab_size, d_model, mask_zero=True)(inputs)\n",
        "_,state_h,state_c = tf.keras.layers.LSTM(d_model,activation='relu',return_state=True)(x)\n",
        "\n",
        "#Decoder\n",
        "targets = tf.keras.layers.Input(shape=(None,))\n",
        "embedding_layer = tf.keras.layers.Embedding(hindi_vocab_size, d_model, mask_zero=True)\n",
        "x = embedding_layer(targets)\n",
        "decoder_lstm = tf.keras.layers.LSTM(d_model,activation='relu',return_sequences=True, return_state=True)\n",
        "x,_,_ = decoder_lstm(x, initial_state=[state_h, state_c])\n",
        "dense1 = tf.keras.layers.Dense(hindi_vocab_size, activation='softmax')\n",
        "x = dense1(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inputs, targets],outputs=x)\n",
        "model.summary()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nq3TUpTK3TRE"
      },
      "outputs": [],
      "source": [
        "#Save model after each epoch\n",
        "save_model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=Weights_DIR+\"\\\\model\\\\\",\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TxsPj7SL1f-b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "284/284 [==============================] - ETA: 0s - loss: 1.9838 - accuracy: 0.2899WARNING:tensorflow:From C:\\Users\\Abhishek pandir\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From C:\\Users\\Abhishek pandir\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: c:\\Users\\Abhishek pandir\\ML\\EnglishHindiTranslationNLP\\weights\\model\\assets\n",
            "284/284 [==============================] - 53s 186ms/step - loss: 1.9838 - accuracy: 0.2899 - val_loss: 1.5494 - val_accuracy: 0.3075\n",
            "Epoch 2/3\n",
            "158/284 [===============>..............] - ETA: 16s - loss: 1.5475 - accuracy: 0.3735"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-17-fc54ec95321d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_model_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTerminateOnNaN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=3, validation_split=validation_split, callbacks=[save_model_callback, tf.keras.callbacks.TerminateOnNaN()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zo7FEy72iWIy"
      },
      "outputs": [],
      "source": [
        "model.save(Weights_DIR+\"\\\\model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx0r37V9fshH",
        "outputId": "e85ee9ae-d693-46c1-f3ee-c52e677a00d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    365568      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    473344      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 1849)   475193      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,364,729\n",
            "Trainable params: 2,364,729\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Retrieve previously saved stuff\n",
        "saved_model = tf.keras.models.load_model(Weights_DIR+\"\\\\model.h5\")\n",
        "\n",
        "saved_model.summary()\n",
        "\n",
        "inputs = saved_model.get_layer('input_1').output\n",
        "_,state_h,state_c = saved_model.get_layer('lstm').output\n",
        "targets = saved_model.get_layer('input_2').output\n",
        "embedding_layer = saved_model.get_layer('embedding_1')\n",
        "decoder_lstm = saved_model.get_layer('lstm_1')\n",
        "dense1 = saved_model.get_layer('dense')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBvZo1SRzHbU",
        "outputId": "fe984e17-b65b-445f-89fc-6e1b10dbca37"
      },
      "outputs": [],
      "source": [
        "#Inference Model\n",
        "\n",
        "#Encoder\n",
        "encoder = tf.keras.models.Model(inputs, [state_h, state_c])\n",
        "\n",
        "#Decoder\n",
        "decoder_input_h = tf.keras.layers.Input(shape=(d_model,))\n",
        "decoder_input_c = tf.keras.layers.Input(shape=(d_model,))\n",
        "x = embedding_layer(targets)\n",
        "x, decoder_output_h, decoder_output_c = decoder_lstm(x, initial_state=[decoder_input_h, decoder_input_c])\n",
        "x = dense1(x)\n",
        "decoder = tf.keras.models.Model([targets] + [decoder_input_h, decoder_input_c], \n",
        "                                [x] + [decoder_output_h, decoder_output_c])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    473344      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[1][0]                \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 1849)   475193      lstm_1[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,473,849\n",
            "Trainable params: 1,473,849\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DM4WlWIkXbS4",
        "outputId": "c1cfe97b-c0d7-4264-bb12-ea7cc0622f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 260, 91, 21, 119, 476]]\n",
            "[[0.01981037 0.00133565]] [2.] [[0.0428213  0.00343366]]\n",
            "[[0.01179558 0.00039951]] [7.] [[0.03238681 0.00165538]]\n",
            "[[0.00087991 0.00012648]] [26.] [[0.00734414 0.00050285]]\n",
            "[[0.00030256 0.00863845]] [26.] [[0.00162887 0.0265565 ]]\n",
            "[[6.6287947e-05 1.2668375e-02]] [14.] [[0.00034521 0.03790924]]\n",
            "[[9.8019900e-06 2.5867904e-03]] [15.] [[7.5834236e-05 1.0165131e-02]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' एक खाली खाली ले जाएँ'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict_sentence(en_input):\n",
        "  input_seq = en_tokenizer.texts_to_sequences([en_input])\n",
        "  next_h, next_c = encoder.predict(input_seq)\n",
        "  \n",
        "  curr_token = np.zeros(1)\n",
        "  curr_token[0] = hi_tokenizer.word_index['<START>']\n",
        "  \n",
        "  pred_sentence = ''\n",
        "  \n",
        "  for i in range(maxlen):\n",
        "    print()\n",
        "    output, next_h, next_c = decoder.predict([curr_token] + [next_h, next_c])\n",
        "    next_token = np.argmax(output[0, 0, :])\n",
        "    next_word = hi_tokenizer.index_word[next_token]\n",
        "    if next_word == '<END>':\n",
        "      break\n",
        "    else:\n",
        "      pred_sentence += ' ' + next_word\n",
        "      curr_token[0] = next_token\n",
        "\n",
        "  return pred_sentence\n",
        "\n",
        "predict_sentence(\"Give your application an accessibility workout\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pq5OesbI4pd7",
        "outputId": "d8d9dd4c-7f57-4077-92e8-a40ece8d9e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> give your application an accessibility workout <END>\n",
            "Prediction:   a को एक खाली खाँचा\n",
            "Dataset Reference:  अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> accerciser accessibility explorer <END>\n",
            "Prediction:   एक खाली पत्ता चलें\n",
            "Dataset Reference:  एक्सेर्साइसर पहुंचनीयता अन्वेषक\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> the default plugin layout for the bottom panel <END>\n",
            "Prediction:   s हेतु s के लिए अंतर्क्रियात्मक खाका\n",
            "Dataset Reference:  निचले पटल के लिए डिफोल्ट प्लगइन खाका\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> the default plugin layout for the top panel <END>\n",
            "Prediction:   s हेतु s के लिए अंतर्क्रियात्मक खाका\n",
            "Dataset Reference:  ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> highlight duration <END>\n",
            "Prediction:   ताश को हाइलाइट करें\n",
            "Dataset Reference:  अवधि को हाइलाइट रकें\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> highlight border color <END>\n",
            "Prediction:   a को एक पत्ता पत्ता ले जाएँ\n",
            "Dataset Reference:  सीमांत बोर्डर के रंग को हाइलाइट करें\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> the color and opacity of the highlight border <END>\n",
            "Prediction:   हाइलाइट बक्से का रंग और रंग\n",
            "Dataset Reference:  हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता।\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> highlight fill color <END>\n",
            "Prediction:   a को एक पत्ता पत्ता ले जाएँ\n",
            "Dataset Reference:  भराई के रंग को हाइलाइट करें\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> the color and opacity of the highlight fill <END>\n",
            "Prediction:   हाइलाइट बक्से का रंग और रंग\n",
            "Dataset Reference:  हाइलाइट किया गया भराई का रंग और पारदर्शिता।\n",
            "\n",
            "1\n",
            "1\n",
            "Input:  <START> api browser <END>\n",
            "Prediction:   फाइल\n",
            "Dataset Reference:  एपीआई विचरक\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> browse the various methods of the current accessible <END>\n",
            "Prediction:   a को एक खाली खाँचा\n",
            "Dataset Reference:  इस समय जिसे प्राप्त किया गया हो उसकी विभिन्न विधियों मेथड में विचरण करें\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> hide private attributes <END>\n",
            "Prediction:   s s\n",
            "Dataset Reference:  निजी गुणों को छिपाएं\n",
            "\n",
            "1\n",
            "1\n",
            "Input:  <START> method <END>\n",
            "Prediction:   number\n",
            "Dataset Reference:  विधि\n",
            "\n",
            "1\n",
            "Input:  <START> property <END>\n",
            "Prediction:  \n",
            "Dataset Reference:  गुणधर्म\n",
            "\n",
            "1\n",
            "1\n",
            "Input:  <START> value <END>\n",
            "Prediction:   मान\n",
            "Dataset Reference:  मान\n",
            "\n",
            "1\n",
            "1\n",
            "Input:  <START> ipython console <END>\n",
            "Prediction:   फाइल\n",
            "Dataset Reference:  आईपाइथन कन्सोल\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> interactive console for manipulating currently selected accessible <END>\n",
            "Prediction:   s हेतु “ s” के लिए अंतर्क्रियात्मक अंतर्क्रियात्मक खाका\n",
            "Dataset Reference:  इस समय चुने गए एक्सेसेबेल से काम लेने के लिए अंतर्क्रियात्मक कन्सोल\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> event monitor <END>\n",
            "Prediction:   कोई दृश्य नहीं\n",
            "Dataset Reference:  घटना मानिटर\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "Input:  <START> monitor events <END>\n",
            "Prediction:   कोई संकेत नहीं है\n",
            "Dataset Reference:  घटनाओं को मानिटर करें m\n",
            "\n",
            "1\n",
            "1\n",
            "Input:  <START> c lear selection <END>\n",
            "Prediction:   प्रोजेक्ट\n",
            "Dataset Reference:  चुनाव को हटाएं c\n",
            "\n",
            "1.9028131210881803e-78\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "#Testing and Analysis\n",
        "import nltk\n",
        "\n",
        "candidates = []\n",
        "references = []\n",
        "\n",
        "ctr = 20 \n",
        "i = 0\n",
        "\n",
        "while ctr>0:\n",
        "  l = len(X_test[i].split())\n",
        "  if l<=maxlen:   #Choose only sentences of length in range [5,15]\n",
        "    pred_sentence = predict_sentence(X_test[i])\n",
        "    candidates.append(pred_sentence.split())\n",
        "\n",
        "    print(\"Input: \", X_test[i])\n",
        "    print(\"Prediction: \", pred_sentence)\n",
        "\n",
        "    # google_translated_sentence = translate_client.translate(X_test[i], target_language='hi')['translatedText']\n",
        "    \n",
        "    # print(\"Google Translated Reference: \", google_translated_sentence)\n",
        "    print(\"Dataset Reference: \", ' '.join(y_test[i].split()[1:-1]))\n",
        "    print()\n",
        "    references.append([y_test[i].split()[1:-1]])\n",
        "\n",
        "    ctr -= 1\n",
        "  i += 1\n",
        "\n",
        "print(nltk.translate.bleu_score.corpus_bleu(references, candidates))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgblcYZIllR6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Neural Machine Translation using LSTM.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9 ('tfgpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "6122a2b3a9f68c9ef77865641a8f411721785de68befcc578f4de469dfd5f506"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
