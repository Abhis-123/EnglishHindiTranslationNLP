{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfxt_CbzXfuX"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhmZMXygmDYL",
        "outputId": "31ef2f80-810b-46c6-a0b9-d41ddf465b21"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpvLb6_koYp0",
        "outputId": "06916ab1-1acc-4793-88b2-4012d8f7f70d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/EnglishHindiTranslationNLP\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/EnglishHindiTranslationNLP\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ3NxGEDnw10"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MucBIk3IpqVk"
      },
      "source": [
        "## Helper "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9EoNM-QpnS9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "def save_variable(variable, file_path):\n",
        "    with open(file_path, 'wb') as handle:\n",
        "        pickle.dump(variable, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# loading\n",
        "def load_variable(file_path):\n",
        "    with open(file_path, 'rb') as handle:\n",
        "        variable = pickle.load(handle)\n",
        "    return variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VuSLHypu4J"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhoLQDkGgdx8"
      },
      "outputs": [],
      "source": [
        "#Make imports\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import seaborn as sns\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from dataset import prepare_data\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6snx-2pS5qD"
      },
      "source": [
        "## Define Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1ZjTK8VvFlN"
      },
      "outputs": [],
      "source": [
        "#Some parameters\n",
        "vocab_size = 10000\n",
        "total_sentences = 25000\n",
        "maxlen = 16\n",
        "epochs = 70\n",
        "validation_split = 0.05\n",
        "max_sentence_length= maxlen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pe7LVRkTAOj"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyHjYB_eMpD7",
        "outputId": "5aae10de-3624-4d25-8007-c1ccc2171167"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25000it [00:00, 181172.24it/s]\n"
          ]
        }
      ],
      "source": [
        "train = prepare_data(type='train', max_entries=total_sentences)\n",
        "en_data = []\n",
        "hi_data = []\n",
        "cnt = 0\n",
        "for (en,hi) in tqdm(zip(train['en'].to_list(), train['hi'].to_list())):\n",
        "  l = min(len(en.split()), len(hi.split()))\n",
        "  if l <= maxlen:\n",
        "    en_data.append(en)\n",
        "    hi_data.append(hi)\n",
        "    cnt += 1\n",
        "  if cnt == total_sentences:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9JE6vrETCqZ"
      },
      "source": [
        "## Tokenize Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwR2rlOvB-xZ",
        "outputId": "719959d8-a9ca-4110-9009-8bf92c408d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Vocab Size:  1922\n",
            "Hindi Vocab Size:  2400\n"
          ]
        }
      ],
      "source": [
        "#Tokenize the texts and convert to sequences\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
        "en_tokenizer.fit_on_texts(en_data)\n",
        "en_sequences = en_tokenizer.texts_to_sequences(en_data)\n",
        "\n",
        "hi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
        "hi_tokenizer.fit_on_texts(hi_data)\n",
        "hi_sequences = hi_tokenizer.texts_to_sequences(hi_data)\n",
        "\n",
        "english_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "hindi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
        "print(\"English Vocab Size: \", english_vocab_size)\n",
        "print(\"Hindi Vocab Size: \", hindi_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjeGZBSVYKlb"
      },
      "source": [
        "## Pad Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luUa7TE6RYFq"
      },
      "outputs": [],
      "source": [
        "#Prepare encoder data\n",
        "encoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(en_sequences, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olIjin62TPF7"
      },
      "outputs": [],
      "source": [
        "#Prepare decoder data\n",
        "decoder_inputs = []\n",
        "decoder_outputs = []\n",
        "\n",
        "for hi in hi_sequences:\n",
        "  decoder_inputs.append(hi[:-1])\n",
        "  decoder_outputs.append(hi[1:])\n",
        "\n",
        "decoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs, maxlen=maxlen, padding='post')\n",
        "decoder_outputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pC7UorhTGsQ"
      },
      "source": [
        "## Test train split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5OWbUrPRENr",
        "outputId": "db1d7570-efcb-4830-9288-741ce1a53d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(22500, 16) (22500, 16) (22500, 16)\n"
          ]
        }
      ],
      "source": [
        "# Training and Testing split\n",
        "# 95%, 5%\n",
        "split = int(0.9 * total_sentences)\n",
        "\n",
        "X_train = [encoder_inputs[:split], decoder_inputs[:split]]\n",
        "y_train = decoder_outputs[:split]\n",
        "\n",
        "# Test data to evaluate our NMT model using BLEU score\n",
        "X_test = en_data[:split]\n",
        "y_test = hi_data[:split]\n",
        "\n",
        "print(X_train[0].shape, X_train[1].shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGYdd9YTTJmT"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUboXx8WjDnP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "class LSTMModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,encoder_vocab_size = None, decoder_vocab_size = None, embedding_size = 128,*args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.encoder_vocab_size = encoder_vocab_size\n",
        "        self.decoder_vocab_size = decoder_vocab_size\n",
        "        self.embedding_size     = embedding_size\n",
        "        # encoder\n",
        "        self.input_1      = tf.keras.layers.InputLayer(input_shape=(None,), name = 'input_1')\n",
        "        self.embedding_1  =   tf.keras.layers.Embedding(encoder_vocab_size, embedding_size,mask_zero=True, name = 'embedding_1')\n",
        "        self.encoder_lstm =   tf.keras.layers.LSTM(embedding_size, return_state=True,  name ='encoder_lstm' )\n",
        "        # decoder \n",
        "        self.input_2      = tf.keras.layers.InputLayer(input_shape=(None,), name='input_2')\n",
        "        self.embedding_2  = tf.keras.layers.Embedding(decoder_vocab_size, embedding_size,mask_zero=True,name= \"embedding_2\")\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(embedding_size,activation='relu',return_sequences=True, return_state=True, name ='decoder_lstm' )\n",
        "\n",
        "        self.token_layer = tf.keras.layers.Dense(decoder_vocab_size,activation='softmax', name = 'token_layer')\n",
        "\n",
        "    def call(self,inputs):\n",
        "        encoder_input = self.input_1(inputs[0])\n",
        "        decoder_input = self.input_2(inputs[1])\n",
        "        # encode the inputs \n",
        "        encoder_embed = self.embedding_1(encoder_input)\n",
        "        # run rnn on the encoded sequence\n",
        "        _, state_h, state_c = self.encoder_lstm(encoder_embed)\n",
        "        # decode the target \n",
        "        decoder_embed = self.embedding_2(decoder_input)\n",
        "        x, _,_ = self.decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
        "        return self.token_layer(x)\n",
        "  \n",
        "    # def get_config(self):\n",
        "    #   config = super.get_config()\n",
        "    #   config['encoder_vocab_size'] = self.encoder_vocab_size\n",
        "    #   config['decoder_vocab_size'] = self.decoder_vocab_size\n",
        "    #   config['embedding_size']     = self.embedding_size\n",
        "    #   return config\n",
        "    def predict_sequence(self,text, input_tokenizer, output_tokenizer, max_len=maxlen):\n",
        "      if type(text)!=list:\n",
        "        text = [text]\n",
        "      input_sequence = input_tokenizer.texts_to_sequences(text)\n",
        "      print(input_sequence)\n",
        "      if type(input_sequence)==list:\n",
        "        input_sequence = np.array(input_sequence)\n",
        "      encoder_embed = self.embedding_1(input_sequence)\n",
        "        # run rnn on the encoded sequence\n",
        "      _, next_h, next_c = self.encoder_lstm(encoder_embed)\n",
        "      curr_token = [[0]]\n",
        "      curr_token[0][0] = output_tokenizer.word_index['<START>']\n",
        "\n",
        "      out_seq = \"\"\n",
        "      for i in range(max_len):\n",
        "        print(curr_token)\n",
        "        decoder_embedding = self.embedding_2(np.array(curr_token))\n",
        "        x, next_h, next_c = self.decoder_lstm(decoder_embedding, initial_state=[next_h, next_c])\n",
        "        x = self.token_layer(x)\n",
        "        next_token = np.argmax(x[0,0,:])\n",
        "        next_word = output_tokenizer.index_word[next_token]\n",
        "        if next_word ==\"<END>\":\n",
        "          break\n",
        "        curr_token[0][0] = next_token\n",
        "        #curr_token[0].append(next_token)\n",
        "        out_seq= out_seq+\" \"+ next_word\n",
        "      return out_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8cs7t8TUgOa",
        "outputId": "705a4f2b-08c1-4a31-ce01-11ac4b794b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer decoder_lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ]
        },
        {
          "ename": "UnknownError",
          "evalue": "Fail to find the dnn implementation. [Op:CudnnRNNV3]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-80edfa991960>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_vocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menglish_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_vocab_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mhindi_vocab_size\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmax_sentence_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmax_sentence_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-9-921e0a01a2c6>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mencoder_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# run rnn on the encoded sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_embed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# decode the target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mdecoder_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcan_use_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m           last_output, outputs, new_h, new_c, runtime = gpu_lstm(\n\u001b[1;32m-> 1177\u001b[1;33m               **gpu_lstm_kwargs)\n\u001b[0m\u001b[0;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mgpu_lstm\u001b[1;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths)\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[0mrnn_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lstm'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[0msequence_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m         time_major=time_major)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgo_backwards\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       outputs = array_ops.reverse_sequence_v2(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnnv3\u001b[1;34m(input, input_h, input_c, params, sequence_lengths, rnn_mode, input_mode, direction, dropout, seed, seed2, num_proj, is_training, time_major, name)\u001b[0m\n\u001b[0;32m   1916\u001b[0m           \u001b[0mrnn_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnn_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m           \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_proj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_proj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m           is_training=is_training, time_major=time_major, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   1919\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m       \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnnv3_eager_fallback\u001b[1;34m(input, input_h, input_c, params, sequence_lengths, rnn_mode, input_mode, direction, dropout, seed, seed2, num_proj, is_training, time_major, name, ctx)\u001b[0m\n\u001b[0;32m   2010\u001b[0m   \"num_proj\", num_proj, \"is_training\", is_training, \"time_major\", time_major)\n\u001b[0;32m   2011\u001b[0m   _result = _execute.execute(b\"CudnnRNNV3\", 5, inputs=_inputs_flat,\n\u001b[1;32m-> 2012\u001b[1;33m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[0;32m   2013\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2014\u001b[0m     _execute.record_gradient(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mUnknownError\u001b[0m: Fail to find the dnn implementation. [Op:CudnnRNNV3]"
          ]
        }
      ],
      "source": [
        "model = LSTMModel(encoder_vocab_size=english_vocab_size, decoder_vocab_size= hindi_vocab_size ,embedding_size=64) \n",
        "model(np.array([[[1]*max_sentence_length], [[1]*max_sentence_length]]))\n",
        "model.summary()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq3TUpTK3TRE"
      },
      "outputs": [],
      "source": [
        "#Save model after each epoch\n",
        "save_model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=Weights_DIR+\"\\\\model\\\\\",\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxsPj7SL1f-b",
        "outputId": "2e707a2a-0f04-4bc5-cbc9-5df41f0502fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "334/334 [==============================] - ETA: 0s - loss: 1.4320 - accuracy: 0.2396"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=10,batch_size= 64, validation_split=validation_split, callbacks=[save_model_callback, tf.keras.callbacks.TerminateOnNaN()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo7FEy72iWIy"
      },
      "outputs": [],
      "source": [
        "model.save_weights(Weights_DIR+\"\\\\model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnZiJq9j1ePH"
      },
      "outputs": [],
      "source": [
        "saved_model = LSTMModel(encoder_vocab_size=english_vocab_size, decoder_vocab_size= hindi_vocab_size ) \n",
        "saved_model(np.array([[[1]*max_sentence_length], [[1]*max_sentence_length]]))\n",
        "saved_model.load_weights(Weights_DIR+\"\\\\model.h5\")\n",
        "saved_model.predict_sequence(\"Hi what is your name\", en_tokenizer, hi_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6uIAnBCQdNa"
      },
      "outputs": [],
      "source": [
        "#Testing and Analysis\n",
        "import nltk\n",
        "\n",
        "candidates = []\n",
        "references = []\n",
        "\n",
        "ctr = 20 \n",
        "i = 0\n",
        "\n",
        "while ctr>0:\n",
        "  l = len(X_test[i].split())\n",
        "  if l<=maxlen:   #Choose only sentences of length in range [5,15]\n",
        "    pred_sentence = predict_sentence(X_test[i])\n",
        "    candidates.append(pred_sentence.split())\n",
        "\n",
        "    print(\"Input: \", X_test[i])\n",
        "    print(\"Prediction: \", pred_sentence)\n",
        "\n",
        "    # google_translated_sentence = translate_client.translate(X_test[i], target_language='hi')['translatedText']\n",
        "    \n",
        "    # print(\"Google Translated Reference: \", google_translated_sentence)\n",
        "    print(\"Dataset Reference: \", ' '.join(y_test[i].split()[1:-1]))\n",
        "    print()\n",
        "    references.append([y_test[i].split()[1:-1]])\n",
        "\n",
        "    ctr -= 1\n",
        "  i += 1\n",
        "\n",
        "print(nltk.translate.bleu_score.corpus_bleu(references, candidates))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq5OesbI4pd7"
      },
      "outputs": [],
      "source": [
        "#Testing and Analysis\n",
        "import nltk\n",
        "\n",
        "candidates = []\n",
        "references = []\n",
        "\n",
        "ctr = 20 \n",
        "i = 0\n",
        "\n",
        "while ctr>0:\n",
        "  l = len(X_test[i].split())\n",
        "  if l<=maxlen:   #Choose only sentences of length in range [5,15]\n",
        "    pred_sentence = predict_sentence(X_test[i])\n",
        "    candidates.append(pred_sentence.split())\n",
        "\n",
        "    print(\"Input: \", X_test[i])\n",
        "    print(\"Prediction: \", pred_sentence)\n",
        "\n",
        "    # google_translated_sentence = translate_client.translate(X_test[i], target_language='hi')['translatedText']\n",
        "    \n",
        "    # print(\"Google Translated Reference: \", google_translated_sentence)\n",
        "    print(\"Dataset Reference: \", ' '.join(y_test[i].split()[1:-1]))\n",
        "    print()\n",
        "    references.append([y_test[i].split()[1:-1]])\n",
        "\n",
        "    ctr -= 1\n",
        "  i += 1\n",
        "\n",
        "print(nltk.translate.bleu_score.corpus_bleu(references, candidates))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgblcYZIllR6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dfxt_CbzXfuX",
        "MucBIk3IpqVk",
        "F5VuSLHypu4J"
      ],
      "name": "Neural_Machine_Translation_using_LSTM.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9 ('tfgpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "6122a2b3a9f68c9ef77865641a8f411721785de68befcc578f4de469dfd5f506"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
